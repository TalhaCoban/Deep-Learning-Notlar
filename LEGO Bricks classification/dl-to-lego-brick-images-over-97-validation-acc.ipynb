{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kernel i trained lego-brick-images but only in \"LEGO brick images v1\" file using keras library.\nTraining procces laid out under three main title ; importing images and label, preprocess and training."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport random\nimport ntpath\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras.utils.np_utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"IMPORTING IMAGES AND LABELS"},{"metadata":{},"cell_type":"markdown","source":"Here we access the images file path. Every files name here will be labels of our images because every image in a file whose name is images belogs to it."},{"metadata":{"trusted":true},"cell_type":"code","source":"basepath = \"/kaggle/input/lego-brick-images/LEGO brick images v1\"\nimg_file_paths = []\nfor _, i, _ in os.walk(basepath):\n    for j in i:\n        img_file_paths.append(os.path.join(basepath, j))\n\nfor img_file_path in img_file_paths:\n    print(img_file_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to open every image in every files in file \"LEGO brick images v1\" with opencv and file index of opened images are going to be its labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"label_index = 0\nimg_labels = []\nimg_dataset = []\nfor img_file_path in img_file_paths:\n    _, label = ntpath.split(img_file_path)\n    print(label)\n    for _,_,image_names in os.walk(img_file_path):\n        for image_name in image_names:\n            image = os.path.join(img_file_path, image_name)           \n            img_labels.append(label_index)\n            image = cv2.imread(image)\n            img_dataset.append(image)\n    label_index += 1\nimg_labels = np.array(img_labels)\nimg_dataset = np.array(img_dataset)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use this list to named classes\n\nlabel_names = [\n    \"3022 Plate 2x2\",\n    \"32123 half Bush\",\n    \"3004 Brick 1x2\",\n    \"6632 Technic Lever 3M\",\n    \"3040 Roof Tile 1x2x45deg\",\n    \"3069 Flat Tile 1x2\",\n    \"3673 Peg 2M\",\n    \"3713 Bush for Cross Axle\",\n    \"3023 Plate 1x2\",\n    \"18651 Cross Axle 2M with Snap friction\",\n    \"3024 Plate 1x1\",\n    \"3794 Plate 1X2 with 1 Knob\",\n    \"3003 Brick 2x2\",\n    \"11214 Bush 3M friction with Cross axle\",\n    \"3005 Brick 1x1\",\n    \"2357 Brick corner 1x2x2\"\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we check whether the number of images opened and number of labels equal or not.\nAnd then we open a random image with matplolib to check that images opened correctly "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(img_labels.shape)\nprint(img_dataset.shape)\nplt.imshow(img_dataset[random.randint(0, img_dataset.shape[0] - 1), :, :, :])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have shown the number of images and number of labels are equal and images saved correctly. However what if our images were uncorrectly labelled. Lets check this out by showing 5 images of every class, writing its label above and we keep sample count of every class. These sample counts can be useful later ;)"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_of_samples = []\n \nnum_columns = 5\nnum_classes = 16\n \nfig, axs = plt.subplots(nrows=num_classes, ncols=num_columns, figsize=(5, 20))\nfig.tight_layout()\n\nfor col in range(num_columns):\n    for row in range(num_classes):\n        selected_images = img_dataset[img_labels==row]\n        axs[row][col].imshow(selected_images[random.randint(0, len(selected_images) - 1), :, :, :], cmap=plt.get_cmap(\"gray\"))\n        axs[row][col].axis(\"off\")\n        if col == 2:\n            axs[row][col].set_title(str(row) + \" : \" + label_names[row])\n            num_of_samples.append(len(selected_images))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we would see the distribution of samples for every class"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(num_of_samples)\nplt.figure(figsize=(12, 4))\nplt.bar(range(0, num_classes), num_of_samples)\nplt.title(\"Distribution of the training dataset\")\nplt.xlabel(\"Class number\")\nplt.ylabel(\"Number of images\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems to be like we keep images and its labels correctly. After that, we can start preproccessing steps"},{"metadata":{},"cell_type":"markdown","source":"**PREPROCCESSING**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Lets split the images into train , validation and test sets and see how these sets distribute"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid_test, y_train, y_valid_test = train_test_split(img_dataset, img_labels, test_size=0.2, random_state=6)\nX_valid, X_test, y_valid, y_test = train_test_split(X_valid_test, y_valid_test, test_size=0.5, random_state=6)\n\nnum_of_test_samples = []\nnum_of_valid_samples = []\n\nfor i in range(num_classes):\n    test_selected = X_test[y_test==i]\n    valid_selected = X_valid[y_valid==i]\n    num_of_test_samples.append(len(test_selected))\n    num_of_valid_samples.append(len(valid_selected))\n    \nprint(num_of_valid_samples)\nprint(num_of_test_samples)\nfig, axs = plt.subplots(nrows=2, ncols=1, figsize=(8, 8))\naxs[0].bar(range(0, num_classes), num_of_valid_samples)\naxs[0].set_title(\"Distribution of validation dataset\")\naxs[0].set_xlabel(\"Class number\")\naxs[0].set_ylabel(\"Number of images\")\naxs[1].bar(range(0, num_classes), num_of_test_samples)\naxs[1].set_title(\"Distribution of test dataset\")\naxs[1].set_xlabel(\"Class number\")\naxs[1].set_ylabel(\"Number of images\")\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We have 5103 train, 638 validation and 638 test sets as you can see"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, y_train.shape)\nprint(X_valid.shape, y_valid.shape)\nprint(X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Lets preprocess the images and see how images changed"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(img):\n    img = cv2.resize(img, dsize=(96, 96), interpolation = cv2.INTER_AREA)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = cv2.equalizeHist(img)\n    return img\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8,8))\nfig.tight_layout()\naxs[0].imshow(X_train[50], cmap=plt.get_cmap(\"gray\"))\naxs[0].set_title(\"Original Image\")\naxs[0].axis(\"off\")\naxs[1].imshow(preprocessing(X_train[50]), cmap=plt.get_cmap(\"gray\"))\naxs[1].set_title(\"Preprocessed Image\")\naxs[1].axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocess all of our images"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.array(list(map(preprocessing, X_train)))\nX_valid = np.array(list(map(preprocessing, X_valid)))\nX_test = np.array(list(map(preprocessing, X_test)))\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\nX_valid = X_valid.reshape(X_valid.shape[0], X_valid.shape[1], X_valid.shape[2], 1)\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalizing\n\nX_train = X_train / 255\nX_test = X_test / 255\nX_valid = X_valid / 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encoding to the labels\n\ny_train = to_categorical(y_train)\ny_valid = to_categorical(y_valid)\ny_test = to_categorical(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TRAINING**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def LeNet_Model():\n    model = Sequential()\n    model.add(Conv2D(90, kernel_size=(5, 5), input_shape=(96, 96, 1), activation=\"relu\"))\n    model.add(Dropout(rate=0.3))\n    model.add(Conv2D(90, kernel_size=(5, 5), activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(60, kernel_size=(3, 3), activation=\"relu\"))\n    model.add(Dropout(rate=0.2))\n    model.add(Conv2D(60, kernel_size=(3, 3), activation=\"relu\"))  \n    model.add(MaxPooling2D(pool_size = (2, 2)))\n    model.add(Flatten())\n    model.add(Dense(units = 750, activation=\"relu\"))\n    model.add(Dropout(rate=0.5))\n    model.add(Dense(units = 375, activation=\"relu\"))\n    model.add(Dropout(rate=0.4))\n    model.add(Dense(units = num_classes, activation=\"softmax\"))\n    model.compile(Adam(lr = 0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LeNet_Model()\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), batch_size=250, epochs=15, verbose=1, shuffle=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show how our model trained; overfitted or underfitted"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,8))\nfig.tight_layout()\naxs[0].plot(history.history[\"loss\"])\naxs[0].plot(history.history[\"val_loss\"])\naxs[0].legend([\"loss\", \"Val_loss\"])\naxs[0].set_title(\"loss\")\naxs[0].set_xlabel(\"epoch\")\naxs[1].plot(history.history[\"accuracy\"])\naxs[1].plot(history.history[\"val_accuracy\"])\naxs[1].legend([\"accuracy\", \"val_accuracy\"])\naxs[1].set_title(\"accuracy\")\naxs[1].set_xlabel(\"epoch\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Everything seems to be good. Our model reach over %97 validation accuracy without underfitting or overfitting.\nLets see test score."},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test, y_test, verbose = 1)\nprint(\"Test score: \", score[0])\nprint(\"Test accuracy: \", score[1])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}